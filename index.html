<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 30px;
    }
    papertitle {
    font-family: 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Helvetica', Lato, Verdana , sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
	
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Helvetica:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Nithin Varma</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>K Nithin Varma</name>
                  
                </p><p align="">I am a final year dual degree (B.Tech + M.Tech) undergraduate at the Department of
                  <a href="http://www.ee.iitm.ac.in/"> Electrical Engineering</a> at <a href="https://www.iitm.ac.in/">Indian Institute of Technology Madras, Chennai</a>.
				  My primary research interests are Machnine Learning, Information Theory and High Dimensional Statistics.
                  <br>
                  <strong>I am currently applying for MS/PhD positions beginning Fall 2023.</strong>

                </p><p align="center">
			
                <!-- Change particulars -->
                <a href="mailto:nithinvarmak2305@gmail.com">Email</a> &nbsp;/&nbsp
<!--                 <a href="https://scholar.google.co.in/citations?hl=en&authuser=1&user=IWZJF8wAAAAJ">Google Scholar</a> /&nbsp -->
<!--                <a href="https://www.researchgate.net/profile/Sourav_Sahoo4">ResearchGate</a> /&nbsp-->
                <a href="https://www.linkedin.com/in/nithin-varma/"> LinkedIn </a> /&nbsp
                <a href="https://n1thinv.github.io/"> Website </a> /&nbsp
                <a href="https://github.com/n1thinV"> GitHub </a> /&nbsp
                <a href="./files/cv_v3.pdf"> CV </a>
                </p>
              </td>
			  <td width="33%">
			  <img src="./images/profile.jpg" alt="" width="100%" align="top">
			  </td>
				<!-- <td> <img src="./img/20190510_111651.jpg" style="width: 200;"></td></tr>  -->
          </tbody></table>

			<hr>
			
			

           <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading><i>Preprints</i></heading>
                <p>
				<papertitle>Implicit Regularization of Mirror Descent</papertitle>
               <br><strong>K Nithin Varma</strong>, Sahin Lale, and Babak Hassibi<br>
			   <i>In prepartion to be submitted  </i>
               <a href="https://n1thinv.github.io/">(Preprint)</a>
			   </p>
			   <p>
In this work, we look at the implicit regularisation properties for Stochastic Mirror Descent (SMD) under different Bregman potentials in overparameterised linear models. 
Using CGMT we transform the primary optimisation (PO) to a dual form which is easier to analyse. 
We characterse the distribution of the global optima that interpolates the data under common potentials like 1-norm;, 2-norm and infinity-norm , 
which matches emperical results obtained using SMD on PO.
                </p>
			   <p>
               <papertitle>Linear Bandits under Safety Constraints</papertitle>
               <br><strong>K Nithin Varma</strong>, Sahin Lale, and Anima Anandkumar<br>
               <i>In prepartion to be submitted  </i>
               <a href="https://n1thinv.github.io/">(Preprint)</a>
			   </p>
			   <p>
In this work we show sublinear regret bounds for stochastic linear bandits under safety constraints. Previous results show sublinear regret for only linear safety constraints or if the constraint belongs to an RKHS family. 
This work extends the results to much more general nonlinear functions with weaker assumptions. 
The main idea we exploit is that learning the constraint function is only crucial near the boundary of the constraint.
                </p>
			   <p>
			   
               <papertitle>An Erasure Queue-Channel with Feedback: Optimal Transmission Control to Maximize Capacity</papertitle>
               <br><strong>K Nithin Varma</strong> and Krishna Jagannathan<br>
			   <strong><i>Submitted to Information Theory Workshop (ITW) 2023 </i></strong>
               <a href="./files/ITW_2023.pdf">(Preprint)</a>
			   </p>
			   <p>
The Erasure Queue Channel model is motivated from the Quantum Queue Channel, where the erasure probability of bits is waiting time dependent. 
In this work we characterised the optimal transmission policy that maximises capcity and proved that the single threshold bang bang policy is optimal.
 The paper is under review for ITW 2023.
                </p>
			   
	     </td>
            </tr>
          </tbody></table>			
			
<hr>
			
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Relevent Projects</heading>
              </td>
            </tr>
          </tbody></table> 

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>

	              </td><td width="75%" valign="top">
	                <p><papertitle>Stochastic Mirror descent</papertitle>
                        <br>K Nithin Varma / EE5121 Final Project<br>
                        <a href="./files/SMD_slides_ver6.pdf">Slides</a>
				                </p><p></p>
				<p>
				In this project I looked into work on Stochastic Mirror Descent on Overparameterized Nonlinear Models and analysed the results.
				Implemented Stochastic Mirror Descent, and demonstrated implicit regularization properties on Neural Networks. 
				Using limited training data by choosing appropriate Bregman divergence loss functions, showed that overparametrized models can generalize better.
                </p><p></p>
                </p><p></p>
                <p></p>
              </td></tr>
			
			
			
			
 	              </td><td width="75%" valign="top">
	                <p><papertitle>Information theoretic Generalization Bounds</papertitle>
                        <br>K Nithin Varma/EE6143 course Project<br>
                        <a href="./files/EE6143_slides.pdf">Slides</a> 
                </p><p></p>
				<p>
This work looks into literature, which talks about modeling learning algorithms as stochastic channels taking training data as input and giving weight parameters at the output.
 This perspective helps provide information-theoretic bounds for generalization and stability
, and I improved upon existing generalization bounds in the literature using f-divergence measures.
                </p>
				<p>
                </p><p></p>
                <p></p>
              </td>
			  
        </tbody></table>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/3.js?i=5uy7w6rqcsq&amp;b=3&amp;s=0&amp;m=2&amp;cl=ffffff&amp;co=010020&amp;cd=aa0000&amp;v0=60&amp;v1=60&amp;r=1" async="async"></script>
</body></html>
