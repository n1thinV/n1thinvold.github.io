<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 30px;
    }
    papertitle {
    font-family: 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Helvetica', Lato, Verdana , sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
	
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Helvetica:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Nithin Varma</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Nithin Varma</name>
                  
                </p><p align="">I am PhD student jointly advised by Prof. <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> and Prof. <a href="https://www.babak.caltech.edu/">Babak Hassibi</a> in the <a href="http://cms.caltech.edu/">CMS department</a> at <a href="https://www.caltech.edu/">Caltech</a>.
                  My primary research interests are in Machnine Learning, Information Theory and High Dimensional Statistics.
                  <br>
				
				</p>
				Prior to my PhD, I completed my undergraduate studies in the Department of Electrical Engineering at <a href="https://www.caltech.edu/">IIT Madras</a>.
				</p>
				
                </p><p align="center">
			
                <!-- Change particulars -->
                <a href="mailto:nithinvarmak2305@gmail.com">Email</a> &nbsp;/&nbsp
<!--                 <a href="https://scholar.google.co.in/citations?hl=en&authuser=1&user=IWZJF8wAAAAJ">Google Scholar</a> /&nbsp -->
<!--                <a href="https://www.researchgate.net/profile/Sourav_Sahoo4">ResearchGate</a> /&nbsp-->
                <a href="https://www.linkedin.com/in/nithin-varma/"> LinkedIn </a> /&nbsp
                <a href="https://n1thinv.github.io/"> Website </a> /&nbsp
                <a href="https://github.com/n1thinV"> GitHub </a> /&nbsp
                <a href="./files/cv_v3.pdf"> CV </a>
                </p>
              </td>
			  <td width="33%">
			  <img src="./images/profile.jpg" alt="" width="100%" align="top">
			  </td>
				<!-- <td> <img src="./img/20190510_111651.jpg" style="width: 200;"></td></tr>  -->
          </tbody></table>

			<hr>
			
			

           <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading><i>Publications</i></heading>
				<p>
               <papertitle>Linear Bandits under Safety Constraints</papertitle>
               <br><strong>K Nithin Varma</strong>, Sahin Lale, and Anima Anandkumar<br>
               <i>In prepartion to be submitted  </i>
               <a href="https://n1thinv.github.io/">(Preprint)</a>
			   </p>
			   <p>
In this work we show sublinear regret bounds for stochastic linear bandits under safety constraints. Previous results show sublinear regret for only linear safety constraints or if the constraint belongs to an RKHS family. 
This work extends the results to much more general nonlinear functions with weaker assumptions. 
The main idea we exploit is that learning the constraint function is only crucial near the boundary of the constraint.
                </p>
                <p>
				<papertitle>The Asymptotic Distribution of the Stochastic
Mirror Descent Iterates in Linear Models</papertitle>
               <br><strong>K Nithin Varma</strong>, Sahin Lale, and Babak Hassibi<br>
			   <strong><i>Accepted at International Symposium of Information Theory (ISIT) 2023 </i></strong>
               <a href="https://n1thinv.github.io/">(Preprint)</a>
			   </p>
			   <p>
In this work, we look at the implicit regularisation properties for Stochastic Mirror Descent (SMD) under different potentials in overparameterised linear models. 
Using CGMT we transform the primary optimisation (PO) to a dual form which is easier to analyse. 
I have charactersed the distribution of the global optima of the model that interpolates the data under common potentials like 1-norm, 2-norm and infinity-norm , 
which matches emperical results obtained using SMD on PO.
                </p>

			   <p>
			   
               <papertitle>An Erasure Queue-Channel with Feedback: Optimal Transmission Control to Maximize Capacity</papertitle>
               <br><strong>K Nithin Varma</strong> and Krishna Jagannathan<br>
			   <strong><i>Accepted at Information Theory Workshop (ITW) 2023 </i></strong>
               <a href="./files/ITW_2023.pdf">(Preprint)</a>
			   </p>
			   <p>
The Erasure Queue Channel model is motivated from the Quantum Queue Channel, where the erasure probability of bits is waiting time dependent. 
In this work we characterised the optimal transmission policy that maximises capcity and proved that the single threshold bang bang policy is optimal.
                </p>
			   
	     </td>
            </tr>
          </tbody></table>			
			
<hr>
			
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Relevent Projects</heading>
              </td>
            </tr>
          </tbody></table> 

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>

	              </td><td width="75%" valign="top">
	                <p><papertitle>Stochastic Mirror descent</papertitle>
                        <br>K Nithin Varma / EE5121 Final Project<br>
                        <a href="./files/SMD_slides_ver6.pdf">Slides</a>
				                </p><p></p>
				<p>
				In this project I looked into work on Stochastic Mirror Descent on Overparameterized Nonlinear Models and analysed the results.
				Implemented Stochastic Mirror Descent, and demonstrated implicit regularization properties on Neural Networks. 
				Using limited training data by choosing appropriate Bregman divergence loss functions, showed that overparametrized models can generalize better.
                </p><p></p>
                </p><p></p>
                <p></p>
              </td></tr>
			
			
			
			
 	              </td><td width="75%" valign="top">
	                <p><papertitle>Information theoretic Generalization Bounds</papertitle>
                        <br>K Nithin Varma/EE6143 course Project<br>
                        <a href="./files/EE6143_slides.pdf">Slides</a> 
                </p><p></p>
				<p>
This work looks into literature, which talks about modeling learning algorithms as stochastic channels taking training data as input and giving weight parameters at the output.
 This perspective helps provide information-theoretic bounds for generalization and stability
, and I improved upon existing generalization bounds in the literature using f-divergence measures.
                </p>
				<p>
                </p><p></p>
                <p></p>
              </td>
			  
        </tbody></table>
	<script type="text/javascript" src="//rf.revolvermaps.com/0/0/3.js?i=5uy7w6rqcsq&amp;b=3&amp;s=0&amp;m=2&amp;cl=ffffff&amp;co=010020&amp;cd=aa0000&amp;v0=60&amp;v1=60&amp;r=1" async="async"></script>	
</body></html>
